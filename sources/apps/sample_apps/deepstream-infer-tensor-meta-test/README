*****************************************************************************
* Copyright (c) 2019 NVIDIA Corporation.  All rights reserved.
*
* NVIDIA Corporation and its licensors retain all intellectual property
* and proprietary rights in and to this software, related documentation
* and any modifications thereto.  Any use, reproduction, disclosure or
* distribution of this software and related documentation without an express
* license agreement from NVIDIA Corporation is strictly prohibited.
*****************************************************************************

Prequisites:

Please follow instructions in the
apps/sample_apps/deepstream-infer-tensor-meta-test/README on how to install the
prequisites for Deepstream SDK, the DeepStream SDK itself and the apps.

Compilation Steps:
  $ cd apps/sample_apps/deepstream-infer-tensor-meta-test/
  $ make
  $ ./deepstream-infer-tensor-meta-app <h264_elementary_stream>

This document shall describe about the sample deepstream-infer-tensor-meta-test
application.

It is meant for simple demonstration of how to access nvinfer's output tensor
data for DeepStream SDK elements in the pipeline.

This sample creates multiple instances of "nvinfer" element. Each instance of
the "nvinfer" uses TensorRT API to infer on frames/objects. Every
instance is configured through its respective config file and enable
each instance to generate raw tensor data on full frame or object. End users
can read or parse the tensor data by probe the buffer and look though metadata.

For reference, here are the config files used for this sample :
1. The inference for 4-class detector (referred to as pgie in this sample) uses
    dstensor_pgie_config.txt
2. The inference for vehicle color classifier (referred to as sgie1 in this
   sample) uses
    dstensor_sgie1_config.txt
3. The inference for vehicle make classifier (referred to as sgie2 in this
   sample) uses
    dstensor_sgie2_config.txt
4. The inference for vehicle type classifier (referred to as sgie3 in this
   sample) uses
    dstensor_sgie3_config.txt

In this sample, we first create one instance of "nvinfer", referred as the pgie.
This is the first inference and "pgie_pad_buffer_probe" function parse the
tensor output data and get detection objects as "Vehicle , RoadSign, TwoWheeler,
Person". After this we create 3 more instances of "nvinfer" referred to as
sgie1, sgie2, sgie3 respectively. The 3 instances would crop pgie's detected
objects and do secondary inference. "sgie_pad_buffer_probe" function parse all
sgies inference tensor data and classify them for OSD to drawing box and text.
To enable output layers' tensor data, we need set property or attribute
    "output-tensor-meta=true".
In the sample code, We also set attribute "network-type=100" in config file but
this is not mandatory. "output-tensor-meta" will work with nvinfer configured
as detector/classifier as well.
pgie would attach tensor output data into each frame's frame_user_meta_list,
and each sgie would attach tensor output data into each object's
obj_user_meta_list. We can probe GstBuffer and loop through meta list to find
all output layers' buffer. With that, we can parse detection and classficiation
results on each layer's host buffer pointer "out_buf_ptrs_host". We also show
that out_buf_ptrs_dev can be used as well.
Please refer the "pgie_pad_buffer_probe" and "sgie_pad_buffer_probe" functions
in the sample code. For details on the Metadata format, refer to the
file "gstnvdsmeta.h"
